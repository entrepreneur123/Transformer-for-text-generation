# -*- coding: utf-8 -*-
"""Transformer for text processing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RyZlHdKHzz86vttuvzOvdP4S69Tp9Xsp

Components of a transformer




1.   Encoder:process input data
2.   Decoder:Reconstructs the output
3.   Feed-forward Neural Network:Refine understanding
4.   Positional Encoding: Ensure order matters
5.  Multi Head Attention: captures multiple inputs or sentiments

preparing our data :train-test split
"""

sentences  = ["I love this product","This is terrible","could be better",'This is the best']
labels = [1,0,0,1]
train_sentencess  = sentences[:3]
train_labels = labels[:3]
test_sentences = sentences[3:]
test_labels = labels[3:]

"""Building the transformer Model"""

class TransformerEncoder(nn.Module):
  def __init__(self,embed_size,heads,num_layers,dropout):
    super(TransformerEncoder,self).__init__()
    self.encoder = nn.TransfomerEncoder(
        nn.TransfomerEncoderLayer(d_model=embed_size,nhead=heads),
        num_layers = num_layers

    )
    self.fc = nn.Linear(embed_size,2)

  def forward(self, x):
    x  =self.encoder(x)
    x = x.mean(dim=1)
    return self.fc(x)

model = TransformerEncoder(embed_size=512,heads=8,num_layers=3,dropout=0.5)
optimizer = optim.Adam(model.parameters(),lr=0.001)
criterion = nn.CrossEntropyLoss()

"""Training the transformers"""

for epoch in range(5):
  for sentence,label in zip(train_sentences,train_labels):
    tokens = sentence.split()
    data = torch.stack([token_embeddings[token] for token in tokens], dim=1)
    output = model(data)
    loss = criterion(output,torch.tensor([label]))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f'Epoch{epoch},Loss:{loss.item()}')

"""Predicting transformers"""

def predict(sentence):
  model.eval()
  with torch.no_grad():
    tokens = sentence.split()
    data = torch.stack([token_embeddings.get(token,torch.rand((1,512)))
                        for token in tokens ], dim=1)
    output = model(data)
    predicted = torch.argmax(output,dim=1)
    return "positive" if predicted.item() ==1 else "Negative"

sample_sentence = "This product can be better"
print(f"'{sample_sentence}' is {predict(sample_sentence)} ")

class TransformerEncoder(nn.Module):
    def __init__(self, embed_size, heads, num_layers, dropout):
        super(TransformerEncoder, self).__init__()
        # Initialize the encoder
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads),
            num_layers=num_layers)
        # Define the fully connected layer
        self.fc = nn.Linear(embed_size, 2)

    def forward(self, x):
        # Pass the input through the transformer encoder
        x = self.encoder(x)
        x = x.mean(dim=1)
        return self.fc(x)

model = TransformerEncoder(embed_size=512, heads=8, num_layers=3, dropout=0.5)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

#practice
for epoch in range(5):
    for sentence, label in zip(train_sentences, train_labels):
        # Split the sentences into tokens and stack the embeddings
        tokens = sentence.split()
        data = torch.stack([token_embeddings[token] for token in tokens], dim=1)
        output = model(data)
        loss = criterion(output, torch.tensor([label]))
        # Zero the gradients and perform a backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        print(f"Epoch {epoch}, Loss: {loss.item()}")

def predict(sentence):
    model.eval()
    # Deactivate the gradient computations and get the sentiment prediction.
    with torch.no_grad():
        tokens = sentence.split()
        data = torch.stack([token_embeddings.get(token, torch.rand((1, 512))) for token in tokens], dim=1)
        output = model(data)
        predicted = torch.argmax(output, dim=1)
        return "Positive" if predicted.item() == 1 else "Negative"

sample_sentence = "This product can be better"
print(f"'{sample_sentence}' is {predict(sample_sentence)}")

"""Attention mechanism for text processing

1.   Assigns importance to words
2.   Ensures that machine's interpretation aligns with human understanding

Self and multi-head attention


1.   self Attention: assigns significance to words within a sentence


*   The cat , which was on the roof , was scared
*   Linking "was scared" to "The cat"


2. Multi-head Attention:like having multiple spotlights, capturing different facets


*   Understanding "was scared" can relate to
*   "The cat","the roof",or "was on"

Attention Mechanism-setting vocabulary and data
"""

data = ["the cat sat on the mat",...]
vocab = set(''.join(data).split())
word_to_ix = {word:i for i, word in enumerate(vocab)}
ix_to_word = {i:word for word , i in word_to_ix.items()}
pairs = [sentence.split() for sentence in data]
input_data = [[word_to_ix[word] for word in sentence [:-1]] for sentence in pairs]
target_data = [word_to_ix[sentence[-1]] for sentence in pairs]
inputs = [torch.tensor(seq,dtype=torch.long) for seq in input_data]
targes = torch.tensor(target_data,dtype=torch.long)

"""Model Defination

"""

embedding_dim = 10
hidden_dim = 16

class RNNWithAttentionModel(nn.Model):
  def __init__(self):
    super(RNNWithAttentionModel,self).__init__()
    self.embeddings = nn.Embedding(vocab_size,embedding_dim)
    self.rnn = nn.RNN(embedding_dim,hidden_dim,batch_first=True)
    self.attention = nn.Linear(hidden_dim,1)
    self.fc = nn.Linear(hidden_dim,vocab_size)

"""Forward propagation with attention"""

def forward(self,x):
  x = self.embeddings(x)
  out,_ = self.rnn(x)
  attn_weights = torch.nn.functional.softmax(self.attention(out).squeeze(2),dim=1)
  context = torch.sum(attn_weights.unsqueeze(2) * out, dim=1)
  out = self.fc(context)
  return out

def pad_sequences(batch):
  max_len = max([len(seq) for seq in batch])
  return torch.stack([torch.cat([seq, torch.zeros(max_len-len(seq)).long()]) for seq in batch])

"""Training preparation"""

criterion = nn.CrossEntropyLoss()
  attention_model = RNNWithAttentionModel()
  optimizer = torch.optim.Adam(attention_model.parameters(),lr=0.01)
  for epoch in range(300):
    attention_model.train()
    optimizer.zero_grad()
    padded_inputs=  pad_sequences(inputs)
    outputs = attention_model(padded_inputs)
    loss = criterion(outputs,targets)
    loss.backward()
    optimizer.step()

"""Model Evaluation

"""

for input_seq, target in zip(input_data,target_data):
  input_test = torch.tensor(input_seq,dtype=torch.long).unsqueeze(0)
  attention_model.eval()
  attention_output = attention_model(input_test)
  attention_prediction = ix_to_word[torch.argmax(attention_output).item()]
  print(f"\nInput:{' '.join([ix_to_word[ix] for ix in input_seq])}")
  print(f"Target:{ix_to_word[target]}")
  print(f"RNN with Attention prediction:{attention_prediction}")

#practice
class RNNWithAttentionModel(nn.Module):
    def __init__(self):
        super(RNNWithAttentionModel, self).__init__()
        # Create an embedding layer for the vocabulary
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)
        # Apply a linear transformation to get the attention scores
        self.attention = nn.Linear(hidden_dim, 1)
        self.fc = nn.Linear(hidden_dim, vocab_size)
    def forward(self, x):
        x = self.embeddings(x)
        out, _ = self.rnn(x)
        #  Get the attention weights
        attn_weights = torch.nn.functional.softmax(self.attention(out).squeeze(2), dim=1)
        # Compute the context vector
        context = torch.sum(attn_weights.unsqueeze(2) * out, dim=1)
        out = self.fc(context)
        return out

attention_model = RNNWithAttentionModel()
optimizer = torch.optim.Adam(attention_model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()
print("Model Instantiated")

#practice
for epoch in range(300):
    attention_model.train()
    optimizer.zero_grad()
    padded_inputs = pad_sequences(inputs)
    outputs = attention_model(padded_inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()

for input_seq, target in zip(input_data, target_data):
    input_test = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)

    #  Set the RNN model to evaluation mode
    rnn_model.eval()
    # Get the RNN output by passing the appropriate input
    rnn_output = rnn_model(input_test)
    # Extract the word with the highest prediction score
    rnn_prediction = ix_to_word[torch.argmax(rnn_output).item()]

    attention_model.eval()
    attention_output = attention_model(input_test)
    # Extract the word with the highest prediction score
    attention_prediction = ix_to_word[torch.argmax(attention_output).item()]

    print(f"\nInput: {' '.join([ix_to_word[ix] for ix in input_seq])}")
    print(f"Target: {ix_to_word[target]}")
    print(f"RNN prediction: {rnn_prediction}")
    print(f"RNN with Attention prediction: {attention_prediction}")

